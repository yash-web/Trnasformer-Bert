{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfeVFbRX9ldY2v7N+zXpYs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yash-web/Trnasformer-Bert/blob/main/Coursera_DeepLearning_AI_Natural_Language_Processing_Specialization_c1_week1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpO8JG4JyKpk",
        "outputId": "55817b3f-c4bb-4284-9b48-54cbfa29356e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# run this cell to import nltk\n",
        "import nltk\n",
        "from os import getcwd\n",
        "# import w1_unittest\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples\n"
      ],
      "metadata": {
        "id": "WowS9c7dybGk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "JVCTMGqPzbQd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ],
      "metadata": {
        "id": "TEY_0Yd4zh41"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "0aqpYRXzzocY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "\n",
        "    tweet = re.sub(r'[$#]|\\bRT\\b|https?://[^\\s]+', '', tweet)\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = [stemmer.stem(word) for word in tweet_tokens\n",
        "                    if word not in stopwords_english and word not in string.punctuation]\n",
        "\n",
        "    return tweets_clean"
      ],
      "metadata": {
        "id": "LztHv76S0TsB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ],
      "metadata": {
        "id": "zc9D_FQb0ixT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f77JnqwB0jOP",
        "outputId": "ee04e4f8-0c04-41c9-f5fe-f6359ea71e6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "85N0KYo42ytJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def calculate_loss(x, y, theta):\n",
        "    x_dot_theta = np.dot(x, theta)\n",
        "    return -1* (np.dot(y.transpose(),np.log(sigmoid(x_dot_theta))) +np.dot( (1 - y).transpose(), np.log(1 - sigmoid(x_dot_theta))))"
      ],
      "metadata": {
        "id": "k4YqMgsM4ynR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_loss(tmp_X,tmp_Y,np.zeros((3, 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80EvhkEA6ZBx",
        "outputId": "19043823-599c-4842-a8b4-fe2af744c550"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.93147181]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the function\n",
        "# Construct a synthetic test case using numpy PRNG functions\n",
        "np.random.seed(1)\n",
        "# X input is 10 x 3 with ones for the bias terms\n",
        "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
        "# Y Labels are 10 x 1\n",
        "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
        "def gradientDescent(tmp_X, tmp_Y, theta, lr, num_iter):\n",
        "  m=tmp_X.shape[0]\n",
        "  for i in range(num_iter):\n",
        "    get_loss=(1/m)*calculate_loss(tmp_X,tmp_Y,theta)\n",
        "\n",
        "\n",
        "    theta=theta-(lr/m)*np.dot(tmp_X.transpose(),sigmoid(np.dot(tmp_X,theta)-tmp_Y))\n",
        "\n",
        "  return get_loss[0][0],theta\n",
        "# Apply gradient descent\n",
        "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
        "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HCQ9XpQ6juz",
        "outputId": "bf3473ff-4a62-4e45-da03-881c71332204"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 1.11449400.\n",
            "The resulting vector of weights is [-1.43e-06, -0.00057089, -0.00125798]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    # Process the tweet\n",
        "    tweet_words = process_tweet(tweet)\n",
        "    x = np.zeros((1, 3))\n",
        "    # Initialize features\n",
        "    pos_words = 0\n",
        "    neg_words = 0\n",
        "    x[0,0]=1\n",
        "    # Count positive and negative words\n",
        "    for word in tweet_words:\n",
        "        # Check for the count of the word with the positive label '1.0'\n",
        "        x[0,1] += freqs.get((word, 1.0), 0)\n",
        "\n",
        "        # Check for the count of the word with the negative label '0.0'\n",
        "        x[0,2] += freqs.get((word, 0.0), 0)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "J_EecGvc8PL2"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp1= extract_features(train_x[0], freqs)\n",
        "tmp1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROdxTSGwB7AG",
        "outputId": "2c5f17b3-0c88-43ac-f9f2-3b281a4bd456"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.000e+00, 3.133e+03, 6.100e+01]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test 2:\n",
        "# check for when the words are not in the freqs dictionary\n",
        "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
        "print(tmp2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4W_yj47EaKC",
        "outputId": "ff99dd70-97a2-428b-e0b7-c94cbf9029af"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSC5jkpkHiR3",
        "outputId": "eca1b648-7597-4594-8678-155314452e50"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.75753005.\n",
            "The resulting vector of weights is [-3e-07, -0.00034299, -0.00057361]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C4 GRADED FUNCTION: predict_tweet\n",
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet,freqs)\n",
        "\n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "5ldIe3w9Hmm9"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to test your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQVeRHZuI6HW",
        "outputId": "c4b6ef06-d297-4124-851f-2feea665b8f9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy -> 0.483533\n",
            "I am bad -> 0.491057\n",
            "this movie should have been great. -> 0.482385\n",
            "great -> 0.486276\n",
            "great great -> 0.472573\n",
            "great great great -> 0.458912\n",
            "great great great great -> 0.445311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "\n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0)\n",
        "\n",
        "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "\n",
        "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "-0nNWvFTI-Tc"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbOP0CMjJJTY",
        "outputId": "51c10741-1596-4442-9e86-39aad0ad8de4"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change the tweet below\n",
        "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else:\n",
        "    print('Negative sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpAIx_5PJMbB",
        "outputId": "04282256-209e-47b2-f5cb-7b651ffd1abc"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
            "[[0.47224811]]\n",
            "Negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXQZsTbMJZie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}